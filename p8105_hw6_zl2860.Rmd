---
title: "p8105_hw6_zl2860"
author: "Zongchao Liu"
date: "11/16/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(purrr)
library(patchwork)
```

# Probelm 1
## Load data and check data

```{r, message=FALSE}
set.seed(886) # reproducible
birthweight = read_csv('./data/birthweight.csv')
skimr::skim(birthweight)
birthweight =
  birthweight %>%
  mutate(babysex = factor(babysex),
         frace = factor(frace),
         malform = factor(malform),
         mrace = factor(mrace)
         )
```
The dataset has 4342 observations with 20 variables.
The brief summary above shows that there is no missing value in all variables. For regression analysis, we transform `babysex`, `frace`, `malform`, `mfrace` to factors.

## Modeling

The following modeling process is based on an automatic approach (stepwise). 

The process of stepwise regression is:

1. Suppose X1 is the first variable added

2. The next step is to fit all regressions with two X variables, X1 being one of them

3. For each regression a t-statistics of the new X is obtained 

4. The new X variable with the largest t-stats (smallest p-value) is the next candidate

5. If t-stats > some predefined level, the second X is then added

6. If not, the program terminates

7. Suppose x2 was added second; the procedure now examines whether any of the other X variables already in the model should be dropped

8. The X value with the smallest t-stats (highest p-value) is dropped

9. The process above repeats until all the variables have been considered by the computer

```{r}
# select features
step(lm(bwt ~ ., data = birthweight), direction = "both")

# build models with selected features
fit_bw_selected =
  lm(bwt ~ babysex + bhead + blength +delwt +fincome
     + gaweeks + mheight + mrace + parity +
       ppwt + smoken, data = birthweight)
summary(fit_bw_selected) %>%
  broom::tidy() %>%
  knitr::kable()

birthweight %>%
  add_predictions(fit_bw_selected) %>%
  add_residuals(fit_bw_selected) %>%
  ggplot(aes( x = pred, y = resid ) ) +
  geom_point(alpha = .4) +
  theme_bw() +
  labs(title = " Residual vs Fitted Values") +
  theme(plot.title = element_text(hjust =.5)) +
  geom_line(aes(y = 0), color = "blue")
  
  
  
```

The regression result is shown in the table above. The resulting model includes `babysex, bhead, blength, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt, smoken` as the predictors. The redisual plot indicates that the variance of the residuals are constant while the predicted values are approximately higher than 2000. 

## Build two models

The following are the regression results for the two other models:

```{r}
fit_main = lm(bwt ~ blength + gaweeks ,data = birthweight)
fit_inter = lm(bwt ~ bhead * blength * babysex ,data = birthweight)
summary(fit_main) %>%
  broom::tidy()
summary(fit_inter) %>%
  broom::tidy()
```



## Making comparisons in terms of the CV prediction error

```{r, warning=FALSE}
cv_df = 
  crossv_mc(birthweight,1000) %>%
  mutate(train = map(train, as_tibble),
         test = map (test, as_tibble),
         model_selected = map(train, ~lm(bwt ~ babysex + bhead + blength +delwt +fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         model_main = map(train, ~lm(bwt ~ gaweeks , data = .x)),
         model_inter = map(train,~lm(bwt ~ bhead * blength * babysex , data = .x)),
         ) %>%
  mutate(rmse_main = map2_dbl(model_main, test, ~rmse(model = .x, data = .y)),
         rmse_inter = map2_dbl(model_inter, test, ~rmse(model = .x, data = .y)),
         rmse_selected = map2_dbl(model_selected, test, ~rmse(model = .x, data =.y)))

cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(everything(),
               names_to = "model",
               values_to = "rmse",
               names_prefix = "rmse_") %>%
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin() +
  ggsci::scale_fill_lancet() +
  theme_bw() +
  labs(title = "Model Comparison") +
  theme( plot.title = element_text(hjust = .5))



```

The plot above shows the distribution of the RMSE values for each candidate model. By using the corss validation, we see that there is difference in the prediction error (RMSE) among the three different models. The model that only considers the main effects has the highest average prediction error than the others. The model built by stepwise approach (selected model) has the lowest average RMSE. Therefore, based on the criterion of RMSE, we conclude that the model that includes `babysex, bhead, blength, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt, smoken` is the best for prediction among the three models.

# Problem 2

## Load data

```{r, message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

## Create a function for resampling data with replacement

```{r}
boot_sample = 
  function(df){
  sample_frac(df, replace = T)
  }

```

## Bootstrap

```{r, warning=FALSE}
#resample 5000 times
bootstrap_df = 
  tibble(
    sample_id = 1 : 5000,
    sample = rerun(5000,boot_sample(weather_df))
  )

# organize results
estimates =
  bootstrap_df %>%
  mutate(model = map(sample, ~lm(tmax ~ tmin, data = .x)),
         result_r = map(model, broom::glance),
         result_beta = map(model, broom::tidy)) %>%
  select(sample_id,result_r, result_beta) %>%
  unnest() %>%
  mutate(term = recode(term, 
                       "(Intercept)" = "beta_0",
                       "tmin" = "beta_1")) %>%
  janitor::clean_names() %>%
  select(sample_id, r_squared, adj_r_squared,term, estimate) %>%
  pivot_wider( names_from = term,
               values_from = estimate) %>%
  mutate(log_beta = log(beta_1 * beta_0))
```

## Plot the distributions 

```{r,message=FALSE, fig.width=8}
# distribution for beta
plt_beta =
  estimates %>%
  ggplot(aes( x = r_squared, y =..density..)) +
  geom_histogram(fill = "blue", alpha = .4) +
  geom_density(aes( x = r_squared, y = ..density..)) +
  theme_bw() +
  labs(title = "Distribution for log(β̂0β̂1)",
       x = "log(β̂0β̂1)",
       y = " Count") +
  theme(plot.title = element_text(hjust = .5 ))

# distribution for r2
plt_rsq =
  estimates %>%
  ggplot(aes( x = log_beta, y = ..density..)) +
  geom_histogram(fill = "red", alpha = .4) +
  theme_bw() +
  geom_density(aes( x = log_beta, y = ..density..)) +
  labs(title = "Distribution for r̂2",
       x = "r̂2",
       y = "Count") +
  theme(plot.title = element_text(hjust = .5 ))  

plt_rsq + plt_beta
```

From the plot above, we see that the distributions of the r_hat-squared and log(beta_0_hat * beta_1_hat) are both nearly normal

## 95% CI

```{r}
quantile(pull(estimates,log_beta), c(.25, .975))
quantile(pull(estimates,r_squared), c(.25, .975))
```

The 95% confidence interval for log(β̂ 0∗β̂ 1) is (1.997271, 2.05847)

The 95% confidence interval for r̂2 is (0.9061750, 0.9278682)

